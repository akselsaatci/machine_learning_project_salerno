{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "from client import Client, DEFAULT_PORT, JOINTS\n",
    "from ddpg import DDPG  # Make sure DDPG class is properly imported\n",
    "\n",
    "TABLE_HEIGHT = 1.0\n",
    "TABLE_THICKNESS = 0.08\n",
    "TABLE_LENGTH = 2.4\n",
    "TABLE_WIDTH = 1.4\n",
    "\n",
    "\n",
    "class ActionSpace:\n",
    "    def __init__(self, low, high):\n",
    "        self.low = np.array(low)\n",
    "        self.high = np.array(high)\n",
    "        self.shape = self.low.shape\n",
    "\n",
    "ACTION_SPACE = ActionSpace(\n",
    "    #Used Joints: Everything but joint[1]\n",
    "    #Actionspace: 0,2,3,...,10\n",
    "    [-0.1, -0.001, -0.7, -0.005, -0.7, -0.01, -0.7, -0.05, -1.4, -0.1],\n",
    "    [0.5, 0.001, 1.5, 0.005, 0.7, 0.01, 0.7, 0.05, 0.7, 0.1])\n",
    "\n",
    "\n",
    "def get_neutral_joint_position():\n",
    "    jp = [0.0]*JOINTS\n",
    "    jp[0] = -0.2\n",
    "    jp[2] = math.pi\n",
    "    a = math.pi/3.8\n",
    "    jp[3] = 0.1\n",
    "    jp[5] = a\n",
    "    jp[7] = a\n",
    "    jp[9] = math.pi/3.5\n",
    "    jp[10] = math.pi/2\n",
    "    return jp\n",
    "\n",
    "\n",
    "class OrnsteinUhlenbeckNoise:\n",
    "    def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.size = size\n",
    "        self.mu = mu #mean of noise. Initially 0.0\n",
    "        self.theta = theta #decline of exploration. Initially 0.15\n",
    "        self.sigma = sigma #exploraion. Initialy 0.2\n",
    "        self.state = np.ones(self.size) * self.mu\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.size) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gamma = 0.7\n",
    "tau = 0.1\n",
    "hidden_size = [256, 256]\n",
    "num_inputs = 38\n",
    "#Initialize Actor\n",
    "actor = DDPG(gamma=gamma, tau=tau, hidden_size=hidden_size, num_inputs=num_inputs, action_space=ACTION_SPACE)\n",
    "actor.load_checkpoint()\n",
    "def run(cli):\n",
    "\n",
    "\n",
    "    # Initialize Replay Buffer\n",
    "    buffer_size = 10000  # Maximum size of the replay buffer\n",
    "    replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    max_episodes = 5000  # Number of episodes to train\n",
    "    max_steps = 400  # Maximum number of steps per episode\n",
    "    episode_rewards = []  # To store rewards for each episode\n",
    "\n",
    "    # Assume action_space is an instance of ActionSpace with appropriate low and high bounds\n",
    "    action_dim = ACTION_SPACE.low.shape[0]\n",
    "    noise = OrnsteinUhlenbeckNoise(size=action_dim)\n",
    "    for episode in range(max_episodes):\n",
    "        episode_reward = 0\n",
    "        state = cli.get_state()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # Convert to PyTorch tensor\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            reward=0\n",
    "            noise.reset()\n",
    "            # Calculate action\n",
    "            action = actor.calc_action(state,noise)\n",
    "            act_action = action.cpu().numpy()[0]\n",
    "            act_action = calc_action(act_action, state[0][17])\n",
    "\n",
    "            # Perform action in environment\n",
    "            cli.send_joints(act_action)\n",
    "\n",
    "            # Receive next state and reward from environment\n",
    "            next_state = cli.get_state()\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "            reward = get_reward(next_state[0],state[0])  #  function to get reward from environment\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            transition = (state, action,reward, next_state)\n",
    "            replay_buffer.append(transition)\n",
    "\n",
    "            #end episode, if point scored\n",
    "            if state[0][35] < next_state[0][35] or state[0][34] < next_state[0][34]:\n",
    "                break\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Perform DDPG updates if replay buffer is sufficiently large\n",
    "            if len(replay_buffer) > 100:\n",
    "                #print(f\"Training...\")\n",
    "                batch_size = 32\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                # Perform DDPG updates\n",
    "                value_loss, policy_loss = actor.update_params(batch)\n",
    "\n",
    "\n",
    "        #here im thinking about clearing the buffer, so only new values are in there\n",
    "        #replay_buffer.clear()\n",
    "\n",
    "        # Print episode results\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "        # Optionally, save checkpoint\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            actor.save_checkpoint(episode + 1, replay_buffer)\n",
    "            #resetting the noise\n",
    "            noise = OrnsteinUhlenbeckNoise(size=action_dim)\n",
    "\n",
    "    # Optionally, plot episode rewards or perform other analyses\n",
    "    # ...\n",
    "\n",
    "    # Finally, save the last checkpoint\n",
    "    actor.save_checkpoint(max_episodes, replay_buffer)\n",
    "\n",
    "\n",
    "def get_reward(states,old_states):\n",
    "    versor=(states[14]-0.02) **2 + (states[15]-0.90) ** 2 + (states[16]-0.43) ** 2\n",
    "    #calculate pos, but without the Y\n",
    "    posnY = (states[11] - states[17]) ** 2 + 0 * (states[12] - states[18]) ** 2 + (states[13] - states[19]) ** 2\n",
    "    pos = (states[11] - states[17]) ** 2 + (states[12] - states[18]) ** 2 + (states[13] - states[19]) ** 2\n",
    "    bonus = 0\n",
    "    if states[13] < 0: bonus += 5\n",
    "    if pos < 0.05 and states[31] < 1: #if ball is hitting paddle and didn't hit robot before\n",
    "        bonus -= 10\n",
    "    # if ball enters enemy area above plate, but not at game-start\n",
    "    if old_states[32] < states[32] and states[19] > 0 and states[22] != 0:\n",
    "        bonus -= 60\n",
    "    if old_states[33] < states[33] and states[22] != 0:  # if ball hits enemy plate, but not at game-start\n",
    "        bonus -= 80\n",
    "        print(\"Crazy! a Hit!\")\n",
    "    if old_states[34] < states[34]:\n",
    "        bonus -= 120\n",
    "        print(\"Crazy! a Point!\")\n",
    "    # MORE IDEAS FOR BONUS:\n",
    "    # Bonus for good speed after hit\n",
    "    # Bonus adds up if its the second ball hit\n",
    "    #print(f\"Pos: {2*posnY}, Versor: {2*versor}\")\n",
    "    #return reward, if game is still playing\n",
    "    if states[28]:\n",
    "        return -( 2 * versor + 2.5 * posnY + bonus)\n",
    "    else:\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "\n",
    "\n",
    "def calc_action(action, y): #uses the standard position and adds changes\n",
    "    a= get_neutral_joint_position()\n",
    "    # Nets give a value between -1 and 1\n",
    "    # Normalize values\n",
    "    action[0] = action[0] * 0.3 + 0.2\n",
    "    action[1] = action[1] * 0.001\n",
    "    action[2] = action[2] * 1.1 + 0.4\n",
    "    action[3] = action[3] * 0.005\n",
    "    action[4] = action[4] * 0.7\n",
    "    action[5] = action[5] * 0.01\n",
    "    action[6] = action[6] * 0.7\n",
    "    action[7] = action[7] * 0.05\n",
    "    action[8] = action[8] * 1.05 - 0.35\n",
    "    action[9] = action[9] * 0.1\n",
    "\n",
    "    #mapping values on the neutral joints\n",
    "    a[1]= y\n",
    "    a[0]= a[0] + action[0]\n",
    "    for i in range(len(action)-1):\n",
    "        a[i+2]= a[i+2] + action[i+1]\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    name = 'Product Client'\n",
    "    if len(sys.argv) > 1:\n",
    "        name = sys.argv[1]\n",
    "\n",
    "    port = DEFAULT_PORT\n",
    "    if len(sys.argv) > 2:\n",
    "        port = sys.argv[2]\n",
    "\n",
    "    host = 'localhost'\n",
    "    if len(sys.argv) > 3:\n",
    "        host = sys.argv[3]\n",
    "\n",
    "    cli = Client(name, host, port)\n",
    "    run(cli)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
